---
title: "Guide d'identification de biodiversité du Québec - Partie 2"
description: "Est-ce que iNaturalist ou les données GBIF sont pertinentes pour informer le développement d'un guide d'identification des espèces courantes?"
bibliography: ../../posts/ref_blg.bib
csl: ../../posts/evolution.csl
date: "2025-05-24"
categories: ["guide", "biodiversité", "analyse", "code", "données"]
image: "https://marcolivierbeausoleil.wordpress.com/wp-content/uploads/2015/07/cropped-imgp81461.jpg"
execute: 
  message: false
  warning: false
editor_options: 
  chunk_output_type: console
---

## Objectifs du projet

étape 3?

## On ne protège que ce qu'on connaît

Bien qu'il existe plusieurs efforts pour étudier la biodiversité au Canada, il manque encore beaucoup d'information. Et cela est compréhensible : le territoire Canadien est vaste et la distribution des humains dans ses terres est inégale. À l'heure actuelle, il n'existe pas de coordination nationale qui permettrait d'obtenir des données de biodiversité, de les partager et de les interpréter pour faire le portrait de l'état de la biodiversité sur l'ensemble du territoire @gonzalezBiodiversityObservationNetwork2025.

Auparavant, pour obtenir des données de biodiversité, les naturalistes allaient sur le terrain collecter des spécimens en prenant soin d'inscrire la date et le lieu, le nom des organismes, puis la personne qui a ramassé l'échantillon. De nombreuses notes additionnelles sur l'habitat, l'altitude, etc., <a href='https://irbv.umontreal.ca/recherche/collections/herbier-marie-victorin/' target='_blank'>sont ajoutées au besoin</a>. Les expéditions de terrain et la collecte de données de biodiversité peuvent devenir coûteuses pour payer les commodités de l'équipe d'échantillonnage : la nourriture, l'hébergement, le transport, surtout si on a besoin de prendre l'avion, sans compter le matériel nécessaire pour la prise de données!

Heureusement, la démocratisation des technologies facilite la collecte des données de biodiversité. Plus besoin d'être une autorité reconnue pour identifier des organismes ou d'avoir un mécène qui supporte une expédition de collecte de données. Aujourd'hui, les GPS dans les téléphones intelligents permettent d'ajouter une référence géographique aux photos tout en capturant la date et l'heure. En plus, votre observation est associée à votre compte. C'est tout ce dont nous avons besoin pour avoir une donnée minimalement intéressante pour la science, comme les étiquettes dans les musées! C'est le principe directeur des plateformes de science citoyenne pour la collecte de données de biodiversité dont <a href='https://www.inaturalist.org' target='_blank'>iNaturalist</a>, <a href='https://ebird.org/home' target='_blank'>eBird</a>, <a href='Observation.org' target='_blank'>Observation.org</a> et <a href='https://plantnet.org' target='_blank'>Pl\@ntnet</a>. Donc devenir naturaliste aujourd'hui peut commencer par se faire un compte sur une plateforme et de vous intéresser à la vie qui vous entoure.

Imaginez une chercheuse, Paule, qui souhaiterait faire une étude sur la conservation de la grive de Bicknell (*Catharus bicknelli*). Elle pourrait bénéficier des données partagées sur les plateformes de science citoyenne. Par contre, avec toutes ces plateformes de publication de données de biodiversité, comment aller chercher cette information afin de faire les analyses de conservation ? La réponse comporte quatre lettres : GBIF <a href='https://www.gbif.org/fr/what-is-gbif' target='_blank'>GBIF (Global Biodiversity Information Facility)</a>.

Le <a href='https://www.gbif.org' target='_blank'>Système mondial d’information sur la biodiversité (SMIB) ou, en anglais, GBIF (Global Biodiversity Information Facility)</a> est un réseau international et une infrastructure qui cherche à donner accès gratuitement et ouvertement aux données de biodiversité. Et on parle de toutes formes de données : les collections des musées, les herbiers, les fossiles, des données génétiques ou simplement des données de présence d'individus, des données collectées selon des protocoles standardisés, etc. GBIF facilite l'accès aux données de différentes sources (comme les plateformes de science citoyenne, mais beaucoup plus!) et accélère l'analyse de données de biodiversité. C'est comme cela que Paule pourra extraire les données provenant d'<a href='https://www.inaturalist.org' target='_blank'>iNaturalist</a>, <a href='https://ebird.org/home' target='_blank'>eBird</a>, <a href='Observation.org' target='_blank'>Observation.org</a>, <a href='https://plantnet.org' target='_blank'>Pl\@ntnet</a> et beaucoup d'autres, dont des agences gouvernementales et des entreprises! La collecte de données peut donc être facilitée grandement et Paule peut produire son article de conservation.

Comme le dirais Raôul Duguay : Merci Tôulmônd!

## Biodiversité : ça mange quoi en hiver?

<https://www.iso.org/standard/17298>

Définition de biodiversité :

## Collectionner la biodiversité

### C'est quoi une donnée de biodiversité?

Pourquoi c'est important?

### Les occurences (présences)

La présence d'organismes à un endroit et un moment déterminé. Certains protocoles d'échantillonnage ont aussi des absences : parfois les absences sont tout aussi importante que les présences puisque cela permet d'obtenir un peut plus d'information sur le potentiel de qualité d'habitat d'une espèce. Les traces d'organismes vivants comme les empreintes, un nid, un terrier, une cavité creusé par un pic, les marques laissées par l'agrile du Frêne, les galles de certaines plantes dûe à des insectes, un arbre grignoté par un castor, etc. comptent aussi comme la preuve d'une présence d'un organisme, donc c'est une sorte d'occurence. Les restants d'organismes (morts, excréments, coquillages, etc.) ou témoignage de la vie par des traces laisser par les fossiles sont aussi des données fondementales pour faire toutes sortes d'analyse en biogéographie. Imaginez, même les traces de vie sur Mars pourraient faire partie de GBIF [@hurowitzRedoxdrivenMineralOrganic2025]!

L'ADN des organismes ou de l'information génétique sous forme d'ADN se retrouvant dans l'environnement (eDNA ou ADN environnementale). Les collections de musées sont aussi riche pour faire des analyses de présence historique d'organismes ou de faire des comparaisons de changement morphologique des populations entre des populations échantillonnées récemment et des mesures similaire fait sur les organismes dans des collections muséales. Les données collectées automatiquement par des machines (caméra de chasses, enregistreur audio, images de drones, signatures spectrales, etc.)

### Événement d'échantillonnage

L'acquisition de données avec des protocoles standardisé, par exemple si l'effort d'échantillonnage est enregistré, est riche pour les chercheurs puisque cela permet de faire des analyses beaucoup plus intéressante. Par exemple, en ayant l'effort d'échantillonnage, il est possible de comparer des sites et rendre les estimations de biodiversité plus robuste. Cela permet de mieux modéliser l'incertitude sur les résultats et donc d'être plus robuste.

### Des listes et métadonnées

Une liste d'organismes vivant pour un milieu est la plus simple et la première pièce d'information intéressante : une collection de noms d'organismes assemblés selon des critères. Un exemple de liste au Québec est la [Liste de la faune vertévrée du Québec (LFVQ; DQC)](https://www.donneesquebec.ca/recherche/dataset/liste-de-la-faune-vertebree-du-quebec) ou celle de [des champignons du Québec par mycoquébec.org](https://www.mycoquebec.org).

### Extensions

La présence d'un organisme est une pièce d'information de base. Mais il est encore plus pertinent d'obtenir des détails sur chaque organisme. En fait, de savoir qu'une espèce est présente est intéresssant pour faire certaines analyses, comme pour prédire la présence de population d'espèce dans des environnements différents. Par contre, la biodiversité est aussi la variation que *chaque individu apporte* au sein d'une populaiton. En effet, des analyses morphologiques nécessitent des mesures de traits d'organismes (mesures du poids, taille du bec, longueur du corps, etc.). Aussi, des données de mouvement d'individus avec la télémétrie ou des GPS est hautement intéressant pour connaître l'écologie du mouvement des organismes, surtout dans un monde où les environnements sont dynamiques. On peut simplement penser aux modifications du territoire par l'humain dans les derniers 400 ans. Une gravure de Thomas Patten [-@pattenVueOrientaleMontreal1760] montre ce à quoi pouvait ressembler la berge de l'île de Montréal, déjà modifié par les colons, mais loin de l'être comme aujourd'hui.

![Gravure [@pattenVueOrientaleMontreal1760]](https://www.beaux-arts.ca/sites/default/files/styles/ngc_scale_1200/public/12679653.jpg?itok=RocHG5z9&timestamp=1656294282)

\[IMAGE DU PORT DE MONTRÉAL \]

## 3. Données de biodiversité

### Exploration des données du Système mondial d'information sur la biodiversité (GBIF)

https://www.gbif.org/fr/article/5g1AjKfCHKGUoK0woYsAQq/quest-ce-que-le-gbif

Quelques progiciels utilisées pour manipuler les données

Importer des fonctions nécessaire.

```{r prep_path, echo=FALSE}
# Charge selon la position du chemin d'accès 
if (!grepl(pattern = '2025_05_24_Guide_biodiv_qc', x = getwd())) {
  source(file = '.InCubateur/2025_05_24_Guide_biodiv_qc/scripts/00_init/00_initialize.R')
} else {
  source(file = 'scripts/00_init/00_initialize.R')
}
```

### Traitement local des données

Le [portail GBIF](https://www.gbif.org) permet d'extraire facilement les données en applicant différents filtres. Lors de l'extraction des données, il y a un code (`JSON`) qui permet de reproduire exactement le même jeu de données. J'ai filtrer les données pour le Québec en entier (`CAN.11_1` : c'est le Québec) et pour seulement les données de présence (`OCCURRENCE_STATUS == present`, donc en excluant les absences). Le code plus bas est formaté en `JSON` et c'est ce qui est envoyé à GBIF pour retourner les données.

```{json}
{
  "type": "and",
  "predicates": [
    {
      "type": "equals",
      "key": "GADM_GID",
      "value": "CAN.11_1",
      "matchCase": false
    },
    {
      "type": "equals",
      "key": "OCCURRENCE_STATUS",
      "value": "present",
      "matchCase": false
    }
  ]
}
```

Par contre, ce jeu de données contient plus de données que ce dont nous avons besoin pour ce projet. Et cela prend un certain temps pour que GBIF prépare et nous donne accès aux données. Il est possible de filtrer davantage les données directement sur le site de GBIF pour ne prendre que les données pour iNaturalist et le règne (*kindgom*) `Animalia`.

Pour ce projet, je voulais apprendre à utiliser `duckDB` avec R. Pourquoi? Parce que les données GBIF pour l'ensemble du Québec en fichier `CSV` (comma separated values) font plus de 20GB ce qui est beaucoup pour beaucoup d'ordinateur portable! En utilisant `duckDB`, il est possible de faire des requêtes aux données sans les charger au complet en mémoire. Ainsi, les données téléchargé sont pour toutes les occurences présentes au Québec, mais filtré par après pour garder les donnée d'iNaturalist avec un niveau de précision taxonomique à l'espèce et plus bas (variété, etc.), puis le règne des animaux. Un exemple de code plus bas montre comment faire cette filtration.

J'ai téléchargé les données sur un disque externe (chemin d'accès `/Volumes/Disk_fun/gbif_data`) pour éviter d'avoir à garder le gros jeu de données (\>20GB) sur mon ordinateur. Donc on peut lire les données à partir du disque externe, mais exporter le résultat filtré localement. Après l'application de filtres pertinents pour ce projet, le fichier exporté a une taille d'environ 200 MB! Cela est permet de mettre toutes les données en mémoire et permet de faire des calculs plus rapide qu'avec le jeu de données complet. Il serait intéressant de tester avec un 'disque' NVMe SSD ce qui a le potentiel d'être beaucoup plus rapide.

Une fois téléchargé, j'ai manipulé le jeu de données (voir `GBIF_duckdb.R`) avec une connection `duckdb` qui permet de lire et manipuler des jeux de données gigantesque (et qui n'entrerais pas en mémoire, e.g., avec 8GB de RAM). Essentiellement, il faut

1.  ouvrir un connexion à une session `duckdb` : (`con <- dbConnect(duckdb())`). L'objet 'con' sera notre session pour faire des opérations avec `duckdb`, mais en restant dans R et en utilisant dplyr! Donc nous pouvons utiliser `duckdb` tout en utilisant les outils qu'on connaît bien.
2.  lire le fichier `CSV` avec `duckdb` : (`gb_dat <- duckdb::tbl_file(con, path_csv)`)
3.  manipuler les avec `duckdb` via dplyr et utiliser `collect()` pour amener les résultats dans la mémoire vive (RAM) de l'orginateur : (`gb_dat |> filter([...]) |> dplyr::select([...]) |> collect()`)
4.  exporter les données filtrées en mémoire avec en `CSV` : (`write.csv([...])`)

```{r preparation_donnees_gbif, eval=FALSE}
[...]
# Données GBIF pour le Québec en entier ----------------------------------------
path_csv = "/Volumes/g_magni/gbif_data/0047252-250827131500795.csv" # Données de plus de 20GB! 

# 1. ouvrir un connexion à une session `duckdb` --------------------------------
con <- DBI::dbConnect(duckdb()) 

# 2. lire le fichier CSV avec `duckdb` -----------------------------------------
gb_dat <- duckdb::tbl_file(con, path_csv)

# 3. manipuler les avec `duckdb` via dplyr : Filtrer les données
# Point de repère temporel : 
#   - 25 sec avec données sur NVMe PCIE 4. 
#   - 180 sec sur disque externe (hard drive) 
tictoc::tic() # Débuter le minuteur 
# Prendre les données gbif 
inat_dat = gbif_qc |> 
  # Sélectionner seulement les colonnes pertinantes pour un projet  
  dplyr::select(kingdom,
                phylum, class, order, family, species, 
                scientificName, verbatimScientificName,
                decimalLatitude, decimalLongitude, 
                coordinateUncertaintyInMeters, 
                identifiedBy, recordedBy, 
                eventDate, 
                taxonRank, 
                datasetKey,
                license) |> 
  # Filtrer seulement les observations du jeu de données : 
  #   - 'iNaturalist Research-grade Observations' == '50c9509d-22c7-4a22-a47d-8c48425ef4a7'
  filter(datasetKey == '50c9509d-22c7-4a22-a47d-8c48425ef4a7', 
         # prendre seulement les observations au niveau de l'espèce et plus précis
         taxonRank %in% c("SPECIES", "SUBSPECIES", "VARIETY"), 
         # Seulement les animaux 
         kingdom %in% c('Animalia')) |> 
  # Puisque toutes les mêmes données, on peut enlever cette colonne 
  dplyr::select(-kingdom) |> 
# 4. Exportation de inat_dat ---------------------------------------------------
duckplyr::as_duckdb_tibble() |>  
# CSV out pour inat seulement fait environ ~140MB 
duckplyr::compute_csv(path = 'data/partie_2/biodiv/gbif_data/inat_research_grade_obs_animalia.csv') 

tictoc::toc() # Fin du minuteur

[...]

```

Cela donne un `CSV` d'environ 140MB. Ensuite, on peut lire les données comme d'habitude!

```{r lire_donnees_inat_prefiltre}
ginat = read.csv(file = 'data/partie_2/biodiv/gbif_data/inat_research_grade_obs_animalia.csv')
```

Pour citer les données GBIF avec un filtre différent de celui des données GBIF se fait avec le `datasetKey` (dans notre cas, seulement `r unique(ginat$datasetKey)` avec `r nrow(ginat)` occurences) et en utilisant l'outil de [jeux de données dérivés de GBIF](https://www.gbif.org/citation-guidelines#derivedDatasets) et voir aussi cet [article de blogue de GBIF sur les données dérivées](https://data-blog.gbif.org/post/derived-datasets/). Le progiciel R [`rgbif` permet aussi](https://docs.ropensci.org/rgbif/reference/derived_dataset.html) d'enregistrer des données dérivés (ou filtrées) avec les `datasetKey`.

Si vous réutilisez le même filtre et téléchargez les données, il est fort possible que les données soient différentes: en effet, vous auriez les données plus à jour! Dans toutes les recherches, il est une bonne pratique de garder la commande exacte pour filtrer les données. Je garder aussi le numéro de téléchargement (e.g., `0047252-250827131500795`) pour référence future.

```{r preparation_noms_binomial, eval=FALSE}
colnames(ginat)
# Compte le nombre d'entrées (pas nécessairement des espèces avec nom binomiale)
# Prend quelques secondes 
nb_unique_nom_sp = ginat |> 
  # Garde les noms unique seulement
  count(species) 
```

```{r exploration_donnes_biodiv}
# Compte le nombre d'occurence après filtration (pas mal!)
count(ginat)

# Nombre d'espèces unique pour chaque ordre. 
nb_sp_top = ginat |> 
  # Obtenir les espèces distinctes (unique)
  distinct(phylum, class, order, family, species) |> 
  count(phylum, class, order) |> 
  group_by(phylum) |> 
  # Compte le nombre d'espèce par 'order' en gardant 'phylum' et 'class.'  
  slice_max(order_by = n, n = 3)
nb_sp_top
```

```{r extraction_classe_plus_nombreuse}
# Regarder le nombre d'observations dans chaque règne-classe dans la base de données
nb_class = ginat |> 
  count(phylum, class) |> 
  arrange(-n)

# Extraction des classes avec plus de n observations 
class_select = nb_class |> 
  filter(n >= 3000) |> 
  pull(class)
```

```{r top_5_especes_en_n_observations_par_classe_et_ordre}
top_nb_sp_per_class = ginat |> 
  filter(!(class %in% c("Aves")))  |> 
  dplyr::count(class, 
               order, 
               species) |> 
  group_by(class, 
           order) |> 
  filter(class %in% class_select) |> 
  slice_max(n = 5,
            order_by = n)
```

Pour les oiseaux, nous allons utiliser la base de données sous format .parquet directement pour filtrer les données, faire un calcul du compte d'observation de chaque espèce.

```{r AVES_top_5_especes_en_n_observations_par_classe_et_ordre}
AVES_top_nb_sp_per_class = ginat |> 
  filter(class %in% c("Aves")) |>  
  count(class, 
        order, 
        family,
        species)
```

Lire des fichiers .parquet est d'une rapidité incroyable.

```{r}

AVES_top_nb_sp_per_class |> 
  count(order) |> 
  arrange(-n)

AVES_prop_nb_sp_per_class = AVES_top_nb_sp_per_class |> 
  filter(!is.na(order)) |> 
  arrange(-n) |> 
  group_by(class, order) |> 
  group_modify(~ slice_max(.data = .x, prop = .18, order_by = n))

AVES_prop_nb_sp_per_class |> 
  ungroup() |> 
  count(order, family) |> 
  arrange(order, -n)

```

### Compte nombre d'observations iNaturalist selon des taxons

```{r fonction_pour_compte_nb_obs_group_ord}
#' Title
#'
#' @param n 
#' @param var 
#'
#' @returns
#' @export
#'
#' @examples
plot_data <- function(data, n, order_sel = 'Lepidoptera') {
  data_order = data |> 
    filter(order %in% order_sel) 
  
  if (nrow(data_order)==0) {
    stop("Pas de données pour l'ordre sélectionné")
  }
  
  data_order |>  
    group_by(phylum, class, 
             species,
             order) |> 
    summarize(total=sum(cnt)) |> 
    arrange(phylum, class, order, desc(total)) |> 
    ungroup() |> 
    group_by(order) |> 
    group_modify(.f = ~slice_head(.x, n = n)) |> 
    mutate(total_obs = sum(total)) |> 
    ungroup() |> 
    mutate(order_total = forcats::fct_reorder(.f = order, 
                                              .x = total_obs, 
                                              .fun = sum, 
                                              .desc = TRUE), 
           sci_lab = gsub(pattern = ' ', replacement = '\n', x = species),
           sci_nom = forcats::fct_reorder(sci_lab, 
                                          total, .desc = TRUE)) |> 
    # Prendre seuelemnt les n premiers
    ggplot(aes(x=forcats::fct_reorder(species, 
                                      total, .desc = TRUE),
               y = total)) + 
    scale_y_log10()+ 
    scale_x_discrete(guide = guide_axis(angle = 45)) +
    geom_bar(aes(fill=total), stat='identity')+
    facet_wrap(order_total~.,   scales = "free_x")+
    geom_text(aes(label = total), vjust = .5, 
              hjust = 1.2,
              angle = 90, colour = "white") 
  # labs(title = order_sel)
}
```

```{r}
sp_count <- ginat |> 
  count(phylum, class, order, species, name = 'cnt') 

# Compte le nombre d'observation par grand groupe d'organimse
# On peut voir qu'il y a des biais dans les informations rapporté (il y a nettement plus de coléoptère : 'Indeed, to a good approximation, all species are insects!' R. May 1986, https://www.annualreviews.org/docserver/fulltext/ento/63/1/annurev-ento-020117-043348.pdf?expires=1755916267&id=id&accname=guest&checksum=1939D9B95199827819F220903CE2448B)
order_count = ginat |> 
  count(phylum, class,  order, name = 'total_par_groupe') |> 
  arrange(-total_par_groupe) |> 
  collect()

order_check = order_count |> 
  slice_head(n = 10) |> 
  pull(order)

plot_data(data = sp_count, n = 20, order_sel = order_check)
plot_data(data = sp_count, n = 10,  order_sel = 'Coleoptera')
plot_data(data = sp_count, n = 60,  order_sel = 'Passeriformes')
```

```{r}

ginat |> 
  filter(phylum %in% c('Chordata','Arthropoda'), 
         !(class %in% 'Teleostei')) |> 
  group_by(phylum, class, order, species) |> 
  count(name = 'cnt') |> 
  ungroup() |> 
  # Filtre seulement les ordres avec un nombre d'observation précis 
  group_by(order) |> 
  mutate(total_order = sum(cnt)) |> 
  filter(total_order>50) |> 
  # Sommaire taxonomique d'observation
  group_by(phylum, class, 
           species,
           order) |> 
  summarize(total=sum(cnt)) |> 
  arrange(phylum, class, order, desc(total)) |> 
  ungroup() |> 
  # Prendre que les n premiers 
  group_by(order) |> 
  group_modify(.f = ~ slice_head(.x, n = 20)) |> 
  mutate(total_obs = sum(total)) |> 
  ungroup() |> 
  mutate(order_total = forcats::fct_reorder(.f = order, 
                                            .x = total_obs, 
                                            .fun = sum, 
                                            .desc = TRUE), 
         class_total = forcats::fct_reorder(.f = class, 
                                            .x = total_obs, 
                                            .fun = sum, 
                                            .desc = TRUE), 
         sci_lab = gsub(pattern = ' ', replacement = '\n', x = species),
         sci_nom = forcats::fct_reorder(sci_lab, 
                                        total, .desc = TRUE)) |> 
  # Prendre seuelemnt les n premiers
  ggplot(aes(x = sci_nom,
             y = total)) + 
  facet_wrap(class_total + order_total~.,   scales = "free_x")+
  scale_y_log10()+ 
  scale_x_discrete(guide = guide_axis(angle = 45)) +
  geom_bar(aes(fill =total), stat = 'identity')+
  theme(axis.text.x = element_text(size = 2))+
  geom_text(aes(label = total), 
            vjust = .5, 
            size = 3,
            hjust = 1.2,
            angle = 90, colour = "white") 
```
